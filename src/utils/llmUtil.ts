// src/utils/llmUtil.ts
import { createOpenAI } from "@ai-sdk/openai";
import { createAnthropic } from "@ai-sdk/anthropic";
import { CoreMessage, generateText, LanguageModel, tool } from "ai";
import { multiProviderSettings } from "../settings";
import { AI_PROVIDERS } from "../types";
import { GetRoamNotesTool, RoamQuerySchema } from "../tools/getRoamNotes";

interface LLMConfig {
  provider: string;
  model: string;
  apiKey: string;
  baseUrl?: string;
  temperature?: number;
  maxTokens?: number;
}

interface LLMResult {
  text: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  toolResults?: any[];
}

export class LLMUtil {
  /**
   * Define getRoamNotes tool for AI SDK
   */
  static getRoamNotesTool = tool({
    description: `æ£€ç´¢ Roam Research ä¸­çš„ç¬”è®°å†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥æ ¹æ®å¤šç§æ¡ä»¶è·å–ç”¨æˆ·çš„ç¬”è®°æ•°æ®ã€‚

ä½¿ç”¨æŒ‡å—ï¼š
- **è·å–ç‰¹å®šæ—¥æœŸç¬”è®°**ï¼šä½¿ç”¨ date å‚æ•° (YYYY-MM-DD)
- **è·å–æ—¥æœŸèŒƒå›´ç¬”è®°**ï¼šä½¿ç”¨ startDate å’Œ endDate å‚æ•°
- **è·å–ç‰¹å®šé¡µé¢**ï¼šä½¿ç”¨ pageTitle å‚æ•°
- **è·å–å½“å‰æŸ¥çœ‹å†…å®¹**ï¼šä½¿ç”¨ currentPageContext: true
- **æŸ¥æ‰¾å¼•ç”¨å†…å®¹**ï¼šä½¿ç”¨ referencedPage å‚æ•°
- **è·å–ç‰¹å®šå—**ï¼šä½¿ç”¨ blockUid å‚æ•°
- **æœç´¢å†…å®¹**ï¼šä½¿ç”¨ searchTerm å‚æ•°

æœ€ä½³å®è·µï¼š
- å½“ç”¨æˆ·æåˆ°æ—¶é—´ï¼ˆ"æ˜¨å¤©"ã€"æœ¬å‘¨"ã€"ä¸Šä¸ªæœˆ"ï¼‰æ—¶ï¼Œè‡ªåŠ¨è½¬æ¢ä¸ºå¯¹åº”çš„æ—¥æœŸå‚æ•°
- å½“ç”¨æˆ·è¯´"å½“å‰ç¬”è®°"ã€"è¿™ä¸ªé¡µé¢"æ—¶ï¼Œä½¿ç”¨ currentPageContext
- å½“ç”¨æˆ·è¯¢é—®æŸä¸ªæ¦‚å¿µæˆ–é¡¹ç›®æ—¶ï¼Œä½¿ç”¨ referencedPage æŸ¥æ‰¾ç›¸å…³å†…å®¹
- æ€»æ˜¯ä½¿ç”¨ limit å‚æ•°æ¥æ§åˆ¶è¿”å›æ•°é‡ï¼Œé¿å…æ•°æ®è¿‡è½½`,

    parameters: RoamQuerySchema,

    execute: async (params) => {
      try {
        console.log("ğŸ”§ æ‰§è¡Œ getRoamNotes å·¥å…·ï¼Œå‚æ•°ï¼š", params);
        const result = await GetRoamNotesTool.execute(params);
        return result;
      } catch (error: any) {
        console.error("âŒ getRoamNotes å·¥å…·æ‰§è¡Œé”™è¯¯ï¼š", error);
        return JSON.stringify({
          success: false,
          error: error.message,
          summary: "å·¥å…·æ‰§è¡Œå¤±è´¥",
        });
      }
    },
  });

  private static getProviderClient(config: LLMConfig): LanguageModel {
    const { provider, model, apiKey, baseUrl } = config;

    switch (provider) {
      case "openai":
        const openai = createOpenAI({
          baseURL: baseUrl || "https://api.openai.com/v1",
          apiKey: apiKey,
        });
        return openai(model);

      case "anthropic":
        const anthropic = createAnthropic({
          baseURL: baseUrl || "https://api.anthropic.com/v1",
          apiKey: apiKey,
        });
        return anthropic(model);

      case "groq":
        const groq = createOpenAI({
          baseURL: baseUrl || "https://api.groq.com/openai/v1",
          apiKey: apiKey,
        });
        return groq(model);

      case "xai":
        const xai = createOpenAI({
          baseURL: baseUrl || "https://api.x.ai/v1",
          apiKey: apiKey,
          fetch: async (url: RequestInfo | URL, init?: RequestInit) => {
            const response = await fetch(url, {
              ...init,
              headers: {
                ...init?.headers,
                "Content-Type": "application/json",
              },
            });

            if (!response.ok) {
              console.error(
                "xAI API Error:",
                response.status,
                response.statusText
              );
              const errorText = await response
                .text()
                .catch(() => "No error details available");
              console.error("xAI API Error Details:", errorText);
              throw new Error(
                `xAI API Error: ${response.status} ${response.statusText}`
              );
            }

            return response;
          },
        });
        return xai(model);

      default:
        throw new Error(`Unsupported provider: ${provider}`);
    }
  }

  private static convertToAISDKMessages(messages: any[]): CoreMessage[] {
    return messages.map((msg) => ({
      role: msg.role as "user" | "assistant" | "system",
      content: msg.content,
    }));
  }

  static async generateResponse(
    config: LLMConfig,
    messages: any[]
  ): Promise<LLMResult> {
    const { temperature = 0.7, maxTokens = 4000 } = config;

    try {
      const model = this.getProviderClient(config);
      const aiMessages = this.convertToAISDKMessages(messages);

      const systemMessage = messages.find((m) => m.role === "system");
      const conversationMessages = messages.filter((m) => m.role !== "system");

      const result = await generateText({
        model,
        system: systemMessage?.content,
        messages: this.convertToAISDKMessages(conversationMessages),
        temperature,
        maxTokens,
      });

      return {
        text: result.text,
        usage: result.usage && {
          promptTokens: result.usage.promptTokens,
          completionTokens: result.usage.completionTokens,
          totalTokens: result.usage.totalTokens,
        },
      };
    } catch (error: any) {
      throw new Error(`LLM generation failed: ${error.message}`);
    }
  }

  static async generateResponseWithCurrentModel(
    userMessage: string,
    systemMessage: string
  ): Promise<LLMResult> {
    const model = multiProviderSettings.currentModel;
    if (!model) {
      throw new Error(
        "No model selected. Please select a model from the dropdown."
      );
    }

    const providerInfo = await this.getProviderForModel(model);
    if (!providerInfo) {
      throw new Error(
        `Model not found or API key not configured for model: ${model}`
      );
    }

    const config: LLMConfig = {
      provider: providerInfo.provider.id,
      model: model,
      apiKey: providerInfo.apiKey,
      baseUrl: providerInfo.provider.baseUrl,
      temperature: multiProviderSettings.temperature || 0.7,
      maxTokens: multiProviderSettings.maxTokens || 4000,
    };

    const messages = [
      { role: "system", content: systemMessage },
      { role: "user", content: userMessage },
    ];

    // Use tool-enabled generation
    return this.generateResponseWithTools(config, messages);
  }

  /**
   * Generate response with AI SDK tool calling support
   */
  static async generateResponseWithTools(
    config: LLMConfig,
    messages: any[]
  ): Promise<LLMResult> {
    const { temperature = 0.7, maxTokens = 4000 } = config;

    console.log("ğŸ”§ å¼€å§‹å·¥å…·è°ƒç”¨ç”Ÿæˆ - é…ç½®:", {
      provider: config.provider,
      model: config.model,
      temperature,
      maxTokens,
      messageCount: messages.length,
    });

    // Check if model supports tools
    const supportsTools = this.modelSupportsTools(
      config.provider,
      config.model
    );
    console.log(
      `ğŸ”§ æ¨¡å‹å·¥å…·æ”¯æŒæ£€æŸ¥: ${config.provider}/${config.model} -> ${
        supportsTools ? "âœ… æ”¯æŒ" : "âŒ ä¸æ”¯æŒ"
      }`
    );

    try {
      // Ollama doesn't support native tool calling, use simulation
      if (config.provider === "ollama") {
        console.log("ğŸ”§ ä½¿ç”¨ Ollama å·¥å…·æ¨¡æ‹Ÿ");
        return this.simulateToolsForOllama(config, messages);
      }

      // For non-tool-supporting models, fall back to regular generation
      if (!supportsTools) {
        console.warn(`âš ï¸ æ¨¡å‹ ${config.model} ä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œå›é€€åˆ°å¸¸è§„ç”Ÿæˆ`);
        return this.generateResponse(config, messages);
      }

      const model = this.getProviderClient(config);
      const systemMessage = messages.find((m) => m.role === "system");
      const conversationMessages = messages.filter((m) => m.role !== "system");

      console.log("ğŸ”§ ä½¿ç”¨ AI SDK å·¥å…·è°ƒç”¨ç”Ÿæˆå“åº”", {
        systemMessageLength: systemMessage?.content?.length || 0,
        conversationMessageCount: conversationMessages.length,
        hasTools: true,
      });

      const result = await generateText({
        model,
        system: systemMessage?.content,
        messages: this.convertToAISDKMessages(conversationMessages),
        tools: {
          getRoamNotes: this.getRoamNotesTool,
        },
        temperature,
        maxTokens,
        maxSteps: 3, // Allow multiple rounds of tool calling
      });

      console.log("ğŸ”§ AI SDK å·¥å…·è°ƒç”¨ç»“æœï¼š", {
        hasToolResults: !!result.toolResults,
        toolCallCount: result.toolResults?.length || 0,
        textLength: result.text.length,
        usage: result.usage
          ? {
              promptTokens: result.usage.promptTokens,
              completionTokens: result.usage.completionTokens,
              totalTokens: result.usage.totalTokens,
            }
          : "N/A",
      });

      // Log tool results details if any
      if (result.toolResults && result.toolResults.length > 0) {
        result.toolResults.forEach((toolResult, index) => {
          console.log(`ğŸ”§ å·¥å…·ç»“æœ ${index + 1}:`, {
            toolName: toolResult.toolName,
            args: toolResult.args,
            resultLength: JSON.stringify(toolResult.result).length,
          });
        });
      }

      return {
        text: result.text,
        usage: result.usage && {
          promptTokens: result.usage.promptTokens,
          completionTokens: result.usage.completionTokens,
          totalTokens: result.usage.totalTokens,
        },
        toolResults: result.toolResults,
      };
    } catch (error: any) {
      console.error("âŒ AI SDK å·¥å…·è°ƒç”¨ç”Ÿæˆå¤±è´¥ï¼š", {
        provider: config.provider,
        model: config.model,
        error: error.message,
        stack: error.stack,
      });

      // æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œè¿æ¥é—®é¢˜
      if (
        error.message.includes("Failed to fetch") ||
        error.message.includes("ERR_EMPTY_RESPONSE")
      ) {
        if (config.provider === "xai") {
          throw new Error(`xAI (Grok) æœåŠ¡è¿æ¥å¤±è´¥ã€‚å¯èƒ½çš„åŸå› ï¼š
1. xAI API æœåŠ¡æš‚æ—¶ä¸å¯ç”¨
2. ç½‘ç»œè¿æ¥é—®é¢˜
3. API Key é…ç½®é”™è¯¯

å»ºè®®ï¼š
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- éªŒè¯ API Key æ˜¯å¦æœ‰æ•ˆ
- å°è¯•å…¶ä»–æ¨¡å‹ï¼ˆå¦‚ OpenAI æˆ– Anthropicï¼‰
- ç¨åé‡è¯•

åŸå§‹é”™è¯¯: ${error.message}`);
        }
      }

      // Check if it's a tool-related error and fallback
      if (
        error.message.includes("tool") ||
        error.message.includes("function")
      ) {
        console.warn(`âš ï¸ å·¥å…·è°ƒç”¨å¤±è´¥ï¼Œå°è¯•å›é€€åˆ°å¸¸è§„ç”Ÿæˆ: ${error.message}`);
        try {
          return this.generateResponse(config, messages);
        } catch (fallbackError: any) {
          console.error("âŒ å›é€€ç”Ÿæˆä¹Ÿå¤±è´¥ï¼š", fallbackError.message);
          throw new Error(
            `å·¥å…·è°ƒç”¨å’Œå›é€€ç”Ÿæˆéƒ½å¤±è´¥: ${error.message} | å›é€€é”™è¯¯: ${fallbackError.message}`
          );
        }
      }

      throw new Error(`LLM å·¥å…·è°ƒç”¨ç”Ÿæˆå¤±è´¥: ${error.message}`);
    }
  }

  /**
   * Format tool results for better understanding by Ollama models
   */
  static formatToolResultForOllama(toolResult: string): string {
    try {
      const parsed = JSON.parse(toolResult);
      
      console.log("ğŸ”§ å·¥å…·ç»“æœè§£æçŠ¶æ€ï¼š", {
        success: parsed.success,
        notesCount: parsed.notes?.length || 0,
        queryType: parsed.queryInfo?.queryType,
        hasWarnings: parsed.warnings?.length > 0
      });
      
      if (!parsed.success || !parsed.notes || parsed.notes.length === 0) {
        const errorMessage = `æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç¬”è®°å†…å®¹ã€‚${parsed.error || ''}`;
        console.log("ğŸ”§ å·¥å…·ç»“æœï¼šæ— æ•°æ®ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯");
        return errorMessage;
      }

      let formatted = `=== ç¬”è®°æŸ¥è¯¢ç»“æœ ===\n`;
      formatted += `æŸ¥è¯¢ç±»å‹: ${parsed.queryInfo?.queryType || 'æœªçŸ¥'}\n`;
      if (parsed.queryInfo?.sourcePage) {
        formatted += `æŸ¥è¯¢é¡µé¢: ${parsed.queryInfo.sourcePage}\n`;
      }
      formatted += `æ‰¾åˆ°ç»“æœ: ${parsed.notes.length} æ¡\n\n`;
      formatted += `ä»¥ä¸‹æ˜¯å…³äº"${parsed.queryInfo?.sourcePage || 'æŸ¥è¯¢å†…å®¹'}"çš„æ‰€æœ‰ç›¸å…³ç¬”è®°ï¼š\n\n`;

      // Format each note
      parsed.notes.forEach((note: any, index: number) => {
        formatted += `ã€ç¬”è®° ${index + 1}ã€‘`;
        
        if (note.title) {
          formatted += ` æ¥è‡ªé¡µé¢: ${note.title}\n`;
        } else {
          formatted += `\n`;
        }
        
        if (note.content) {
          // Clean up the content - remove excessive whitespace and format nicely
          const cleanContent = note.content
            .replace(/\n\s*\n/g, '\n')
            .replace(/\s+/g, ' ')
            .trim();
          formatted += `å†…å®¹: ${cleanContent}\n`;
        }
        
        if (note.uid) {
          formatted += `å¼•ç”¨ID: ((${note.uid}))\n`;
        }
        
        if (note.hasMore) {
          formatted += `[æ³¨æ„ï¼šå†…å®¹å·²æˆªæ–­ï¼Œå®é™…å†…å®¹æ›´é•¿]\n`;
        }
        
        formatted += `---\n\n`;
      });

      if (parsed.warnings && parsed.warnings.length > 0) {
        formatted += `\næ³¨æ„äº‹é¡¹:\n`;
        parsed.warnings.forEach((warning: string) => {
          formatted += `- ${warning}\n`;
        });
      }

      formatted += `\n=== æ€»ç»“æŒ‡ç¤º ===\n`;
      formatted += `è¯·åŸºäºä»¥ä¸Š ${parsed.notes.length} æ¡ç¬”è®°ï¼Œæ€»ç»“å…³äº"${parsed.queryInfo?.sourcePage || 'æŸ¥è¯¢å†…å®¹'}"çš„ä¸»è¦å†…å®¹ã€‚\n`;
      formatted += `è¿™äº›ç¬”è®°æ¶µç›–äº†ç”¨æˆ·åœ¨ Roam Research ä¸­è®°å½•çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯ã€‚\n`;
      formatted += `è¯·æå–å…³é”®ä¿¡æ¯å¹¶ç»„ç»‡æˆæ¸…æ™°çš„å›ç­”ã€‚\n`;

      console.log("ğŸ”§ å·¥å…·ç»“æœæ ¼å¼åŒ–å®Œæˆï¼š", {
        originalLength: toolResult.length,
        formattedLength: formatted.length,
        notesProcessed: parsed.notes.length,
        hasStructuredFormat: formatted.includes("=== ç¬”è®°æŸ¥è¯¢ç»“æœ ==="),
        includesSummaryInstruction: formatted.includes("=== æ€»ç»“æŒ‡ç¤º ===")
      });

      return formatted;
    } catch (error) {
      console.warn("ğŸ”§ å·¥å…·ç»“æœæ ¼å¼åŒ–å¤±è´¥:", error);
      console.log("ğŸ”§ å›é€€åˆ°åŸå§‹å·¥å…·ç»“æœ");
      return `å·¥å…·æ‰§è¡Œç»“æœ:\n${toolResult}`;
    }
  }

  /**
   * Simulate tools for Ollama (which doesn't support native tool calling)
   */
  static async simulateToolsForOllama(
    config: LLMConfig,
    messages: any[]
  ): Promise<LLMResult> {
    try {
      console.log("ğŸ”§ ä¸º Ollama æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨", {
        model: config.model,
        messageCount: messages.length,
      });

      const userMessage =
        messages.find((m) => m.role === "user")?.content || "";
      const systemMessage =
        messages.find((m) => m.role === "system")?.content || "";

      console.log("ğŸ”§ Ollama æ¶ˆæ¯åˆ†æ:", {
        userMessageLength: userMessage.length,
        systemMessageLength: systemMessage.length,
        userMessagePreview:
          userMessage.substring(0, 100) +
          (userMessage.length > 100 ? "..." : ""),
      });

      // 1. First, check if the user message needs tool calling
      const toolDetectionPrompt = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å…·è°ƒç”¨åˆ†æåŠ©æ‰‹ã€‚è¯·ä»”ç»†åˆ†æç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è·å– Roam ç¬”è®°æ•°æ®ã€‚

ç”¨æˆ·æ¶ˆæ¯ï¼š${userMessage}

=== å·¥å…·è°ƒç”¨åˆ†ææŒ‡å— ===
å¦‚æœç”¨æˆ·æåˆ°ä»¥ä¸‹å†…å®¹ï¼Œåˆ™éœ€è¦è°ƒç”¨å·¥å…·ï¼š
- é¡µé¢åæˆ–å¼•ç”¨ï¼ˆå¦‚ [[é¡µé¢å]]ã€#æ ‡ç­¾ã€æåˆ°ç‰¹å®šäººå/æ¦‚å¿µï¼‰
- æ—¶é—´ç›¸å…³æŸ¥è¯¢ï¼ˆæ˜¨å¤©ã€ä¸Šå‘¨ã€æŸä¸ªæ—¥æœŸçš„ç¬”è®°ï¼‰
- æœç´¢ç‰¹å®šå†…å®¹ï¼ˆåŒ…å«æŸä¸ªå…³é”®è¯çš„ç¬”è®°ï¼‰
- å½“å‰é¡µé¢ç›¸å…³ï¼ˆè¿™ä¸ªé¡µé¢ã€å½“å‰ç¬”è®°ï¼‰

=== å·¥å…·è°ƒç”¨æ ¼å¼ ===
å¦‚æœéœ€è¦å·¥å…·è°ƒç”¨ï¼Œè¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›ï¼š

TOOL_CALL:getRoamNotes
PARAMETERS:{...jsonå‚æ•°...}

å‚æ•°å­—æ®µè¯´æ˜ï¼š
- date: "YYYY-MM-DD" (æŸ¥è¯¢ç‰¹å®šæ—¥æœŸçš„ç¬”è®°)
- startDate + endDate: "YYYY-MM-DD" (æŸ¥è¯¢æ—¥æœŸèŒƒå›´)
- pageTitle: "é¡µé¢æ ‡é¢˜" (æŸ¥è¯¢ç‰¹å®šé¡µé¢)
- referencedPage: "é¡µé¢å" (æŸ¥è¯¢å¼•ç”¨æŸé¡µé¢çš„æ‰€æœ‰å†…å®¹)
- searchTerm: "æœç´¢è¯" (å…¨æ–‡æœç´¢)
- currentPageContext: true (è·å–å½“å‰é¡µé¢å†…å®¹)

=== å‚æ•°é€‰æ‹©ç¤ºä¾‹ ===
- "æ˜¨å¤©çš„ç¬”è®°" â†’ {"date": "2025-07-08"}
- "ä¸Šå‘¨çš„å·¥ä½œ" â†’ {"startDate": "2025-06-30", "endDate": "2025-07-06"}
- "å…³äºé¡¹ç›®çš„ç¬”è®°" â†’ {"referencedPage": "é¡¹ç›®"}
- "çœ‹ä¸€ä¸‹ [[æ›¹å¤§]]" â†’ {"referencedPage": "æ›¹å¤§"}
- "åŒ…å«Goè¯­è¨€çš„å†…å®¹" â†’ {"searchTerm": "Goè¯­è¨€"}

å¦‚æœä¸éœ€è¦å·¥å…·è°ƒç”¨ï¼Œè¯·å›ç­”"ä¸éœ€è¦"ã€‚

è¯·åˆ†æå¹¶è¿”å›æ­£ç¡®çš„å·¥å…·è°ƒç”¨å‚æ•°ï¼š`;

      const detectionResult = await this.handleOllamaRequest(config, [
        { role: "system", content: toolDetectionPrompt },
        { role: "user", content: userMessage },
      ]);

      console.log("ğŸ”§ Ollama å·¥å…·æ£€æµ‹ç»“æœï¼š", detectionResult.text);

      // 2. Parse if there's a tool call
      if (detectionResult.text.includes("TOOL_CALL:getRoamNotes")) {
        try {
          const paramMatch = detectionResult.text.match(
            /PARAMETERS:\s*({.*?})/
          );
          if (paramMatch) {
            let params = JSON.parse(paramMatch[1]);
            console.log("ğŸ”§ è§£æåˆ°åŸå§‹å‚æ•°ï¼š", params);

            // å¤„ç†é”™è¯¯çš„å‚æ•°æ ¼å¼
            if (
              params.query &&
              !params.startDate &&
              !params.date &&
              !params.pageTitle &&
              !params.referencedPage &&
              !params.searchTerm
            ) {
              console.log("ğŸ”§ æ£€æµ‹åˆ°é”™è¯¯çš„ query å‚æ•°ï¼Œè½¬æ¢ä¸ºæ­£ç¡®æ ¼å¼");
              const query = params.query;

              // æ ¹æ®æŸ¥è¯¢å†…å®¹æ¨æ–­æ­£ç¡®çš„å‚æ•°
              if (query.includes("ä¸Šå‘¨") || query.includes("last week")) {
                // è®¡ç®—ä¸Šå‘¨çš„æ—¥æœŸèŒƒå›´
                const today = new Date();
                const lastWeekEnd = new Date(
                  today.getTime() - (today.getDay() + 1) * 24 * 60 * 60 * 1000
                );
                const lastWeekStart = new Date(
                  lastWeekEnd.getTime() - 6 * 24 * 60 * 60 * 1000
                );

                params = {
                  startDate: lastWeekStart.toISOString().split("T")[0],
                  endDate: lastWeekEnd.toISOString().split("T")[0],
                  limit: 10,
                };
              } else if (
                query.includes("æ˜¨å¤©") ||
                query.includes("yesterday")
              ) {
                const yesterday = new Date();
                yesterday.setDate(yesterday.getDate() - 1);
                params = {
                  date: yesterday.toISOString().split("T")[0],
                  limit: 10,
                };
              } else if (query.includes("å·¥ä½œ") || query.includes("é¡¹ç›®")) {
                params = {
                  searchTerm: "å·¥ä½œ",
                  limit: 10,
                };
              } else {
                params = {
                  searchTerm: query,
                  limit: 10,
                };
              }
            }

            console.log("ğŸ”§ æœ€ç»ˆå·¥å…·å‚æ•°ï¼š", params);
            const toolResult = await GetRoamNotesTool.execute(params);
            console.log("ğŸ”§ å·¥å…·æ‰§è¡Œç»“æœï¼ˆåŸå§‹JSONï¼‰ï¼š", {
              length: toolResult.length,
              preview: toolResult.substring(0, 200) + (toolResult.length > 200 ? "..." : ""),
              isValidJson: (() => {
                try {
                  JSON.parse(toolResult);
                  return true;
                } catch {
                  return false;
                }
              })()
            });

            // 3. Format tool result for better understanding
            const formattedToolResult = this.formatToolResultForOllama(toolResult);
            console.log("ğŸ”§ æ ¼å¼åŒ–åçš„å·¥å…·ç»“æœï¼š", {
              length: formattedToolResult.length,
              preview: formattedToolResult.substring(0, 300) + (formattedToolResult.length > 300 ? "..." : ""),
              improvedReadability: formattedToolResult.includes("=== ç¬”è®°æŸ¥è¯¢ç»“æœ ===")
            });

            // 4. Add tool result to context and regenerate
            const enhancedPrompt = `ä½ æ˜¯ Roam Research çš„AIåŠ©æ‰‹ï¼Œå·²ç»æˆåŠŸè·å–äº†ç”¨æˆ·æŸ¥è¯¢çš„ç¬”è®°æ•°æ®ã€‚

=== å·¥å…·è°ƒç”¨ç»“æœ ===
${formattedToolResult}

=== å…³é”®æŒ‡ç¤º ===
ä¸Šé¢çš„å·¥å…·è°ƒç”¨ç»“æœåŒ…å«äº†ç”¨æˆ·æŸ¥è¯¢çš„çœŸå®ç¬”è®°æ•°æ®ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š

1. **å¿…é¡»åŸºäºå·¥å…·ç»“æœå›ç­”**ï¼šåªä½¿ç”¨ä¸Šé¢å·¥å…·è°ƒç”¨ç»“æœä¸­çš„çœŸå®æ•°æ®
2. **æ€»ç»“ä¸»è¦å†…å®¹**ï¼šå¦‚æœæ‰¾åˆ°äº†ç›¸å…³ç¬”è®°ï¼Œè¯·æ€»ç»“å…³é”®ä¿¡æ¯å’Œä¸»è¦å†…å®¹
3. **å¼•ç”¨å…·ä½“å†…å®¹**ï¼šå¯ä»¥å¼•ç”¨å…·ä½“çš„ç¬”è®°å†…å®¹å’ŒUIDï¼ˆå¦‚ ((uid))ï¼‰
4. **è¯šå®å›ç­”**ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·æ˜ç¡®è¯´æ˜
5. **ä¸è¦ç¼–é€ **ï¼šç»ä¸ç¼–é€ æˆ–æ¨æµ‹ä¸å­˜åœ¨çš„ä¿¡æ¯

=== å›ç­”æ ¼å¼ ===
è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
- å…ˆè¯´æ˜æŸ¥è¯¢ç»“æœï¼ˆæ‰¾åˆ°äº†å¤šå°‘æ¡ç›¸å…³ç¬”è®°ï¼‰
- ç„¶åæ€»ç»“ä¸»è¦å†…å®¹
- æœ€åå¯ä»¥å¼•ç”¨å…·ä½“çš„ç¬”è®°å†…å®¹

ç”¨æˆ·é—®é¢˜ï¼š${userMessage}

è¯·åŸºäºä¸Šè¿°å·¥å…·è°ƒç”¨ç»“æœå›ç­”ï¼š`;

            console.log("ğŸ”§ ç”Ÿæˆå¢å¼º promptï¼š", {
              originalSystemMessageLength: systemMessage.length,
              enhancedPromptLength: enhancedPrompt.length,
              includesToolResult: enhancedPrompt.includes("=== å·¥å…·è°ƒç”¨ç»“æœ ==="),
              includesInstructions: enhancedPrompt.includes("=== é‡è¦æŒ‡ç¤º ===")
            });

            const finalResponse = await this.handleOllamaRequest(config, [
              { role: "system", content: enhancedPrompt },
              { role: "user", content: `åŸºäºå·¥å…·è°ƒç”¨ç»“æœå›ç­”ï¼š${userMessage}` },
            ]);

            console.log("ğŸ”§ Ollama æœ€ç»ˆå›ç­”åˆ†æï¼š", {
              responseLength: finalResponse.text.length,
              preview: finalResponse.text.substring(0, 200) + (finalResponse.text.length > 200 ? "..." : ""),
              containsToolData: finalResponse.text.includes("æ›¹å¤§") || finalResponse.text.includes("Go") || finalResponse.text.includes("æ€§èƒ½"),
              respondsToQuery: finalResponse.text.includes("ä¸»è¦å†…å®¹") || finalResponse.text.includes("ç¬”è®°") || finalResponse.text.includes("æ‰¾åˆ°"),
              isGenericResponse: finalResponse.text.includes("æ²¡æœ‰æ˜ç¡®çš„å…¬å¼€ä¿¡æ¯") || finalResponse.text.includes("å¯èƒ½æ˜¯ä»¥ä¸‹æƒ…å†µ")
            });

            return finalResponse;
          }
        } catch (error) {
          console.warn("âŒ Ollama å·¥å…·æ¨¡æ‹Ÿå¤±è´¥ï¼š", error);
        }
      }

      // 4. No tool call needed, process normally
      console.log("ğŸ”§ æ— éœ€å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸å¤„ç†");
      const normalResponse = await this.handleOllamaRequest(config, messages);
      
      console.log("ğŸ”§ Ollama å¸¸è§„å›ç­”åˆ†æï¼š", {
        responseLength: normalResponse.text.length,
        preview: normalResponse.text.substring(0, 200) + (normalResponse.text.length > 200 ? "..." : ""),
        isNormalFlow: true
      });
      
      return normalResponse;
    } catch (error: any) {
      console.error("âŒ Ollama å·¥å…·æ¨¡æ‹Ÿé”™è¯¯ï¼š", error);
      return this.handleOllamaRequest(config, messages);
    }
  }

  static async getProviderForModel(
    model: string
  ): Promise<{ provider: any; apiKey: string } | null> {
    for (const provider of AI_PROVIDERS) {
      if (provider.models.includes(model)) {
        if (provider.id === "ollama") {
          return { provider, apiKey: "" };
        }

        const apiKey = multiProviderSettings.apiKeys[provider.id];
        if (apiKey && apiKey.trim() !== "") {
          return { provider, apiKey };
        }
      }
    }

    const ollamaProvider = AI_PROVIDERS.find((p) => p.id === "ollama");
    if (ollamaProvider?.supportsDynamicModels) {
      try {
        const dynamicModels = await this.getOllamaModels();
        if (dynamicModels.includes(model)) {
          return { provider: ollamaProvider, apiKey: "" };
        }
      } catch (error) {
        console.log("Failed to check Ollama dynamic models:", error);
      }

      console.log(`Assuming model "${model}" is a local Ollama model`);
      return { provider: ollamaProvider, apiKey: "" };
    }

    return null;
  }

  static async getOllamaModels(baseUrl?: string): Promise<string[]> {
    const url =
      baseUrl ||
      multiProviderSettings.ollamaBaseUrl ||
      "http://localhost:11434";

    try {
      const response = await fetch(`${url}/api/tags`, {
        method: "GET",
        headers: {
          "Content-Type": "application/json",
        },
      });

      if (!response.ok) {
        console.warn(`Failed to fetch Ollama models: HTTP ${response.status}`);
        return [];
      }

      const data = await response.json();
      const models = data.models?.map((model: any) => model.name) || [];
      console.log("Ollama models fetched:", models);
      return models;
    } catch (error: any) {
      console.warn("Failed to fetch Ollama models:", error.message);
      return [];
    }
  }

  static async handleOllamaRequest(
    config: LLMConfig,
    messages: any[]
  ): Promise<LLMResult> {
    const baseUrl =
      multiProviderSettings.ollamaBaseUrl || "http://localhost:11434";
    const { model, temperature = 0.7, maxTokens = 4000 } = config;

    try {
      const response = await fetch(`${baseUrl}/api/chat`, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: model,
          messages: messages,
          stream: false,
          options: {
            temperature: temperature,
            num_predict: maxTokens,
          },
        }),
      });

      if (!response.ok) {
        const error = await response.text().catch(() => "");
        throw new Error(
          `Ollama API error: ${response.status} ${error || response.statusText}`
        );
      }

      const data = await response.json();
      const text = data.message?.content || "No response generated";

      return {
        text,
        usage: data.usage && {
          promptTokens: data.usage.prompt_tokens || 0,
          completionTokens: data.usage.completion_tokens || 0,
          totalTokens: data.usage.total_tokens || 0,
        },
      };
    } catch (error: any) {
      if (error.message.includes("fetch")) {
        throw new Error(
          `Cannot connect to Ollama service (${baseUrl}). Please ensure:\n1. Ollama is installed and running\n2. Service URL is configured correctly\n3. Model "${model}" is downloaded`
        );
      }
      throw error;
    }
  }

  static async testOllamaConnection(
    baseUrl?: string
  ): Promise<{ isConnected: boolean; models?: string[]; error?: string }> {
    const url =
      baseUrl ||
      multiProviderSettings.ollamaBaseUrl ||
      "http://localhost:11434";

    try {
      const response = await fetch(`${url}/api/tags`, {
        method: "GET",
        headers: {
          "Content-Type": "application/json",
        },
      });

      if (!response.ok) {
        return {
          isConnected: false,
          error: `HTTP ${response.status}: ${response.statusText}`,
        };
      }

      const data = await response.json();
      const models = data.models?.map((model: any) => model.name) || [];

      return {
        isConnected: true,
        models,
      };
    } catch (error: any) {
      return {
        isConnected: false,
        error: error.message || "Connection failed",
      };
    }
  }

  /**
   * Check if a model supports tool calling
   */
  static modelSupportsTools(provider: string, model: string): boolean {
    // Tool-enabled models by provider
    const toolSupportedModels: { [provider: string]: string[] } = {
      openai: [
        "gpt-4o",
        "gpt-4o-mini",
        "gpt-4-turbo",
        "gpt-4",
        "gpt-3.5-turbo",
      ],
      anthropic: [
        "claude-3-5-sonnet-20241022",
        "claude-3-5-haiku-20241022",
        "claude-3-opus-20240229",
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307",
      ],
      groq: [
        "llama-3.3-70b-versatile",
        "llama-3.1-70b-versatile",
        "llama-3.1-8b-instant",
        "llama3-groq-70b-8192-tool-use-preview",
        "llama3-groq-8b-8192-tool-use-preview",
      ],
      xai: ["grok-3", "grok-3-beta", "grok-2-vision-1212", "grok-2"],
      // Ollama uses simulation, so technically "supports" tools
      ollama: ["*"], // All models via simulation
    };

    const supportedModels = toolSupportedModels[provider];
    if (!supportedModels) {
      console.warn(`ğŸ”§ Unknown provider for tool support check: ${provider}`);
      return false;
    }

    // Ollama supports all models via simulation
    if (provider === "ollama") {
      return true;
    }

    const isSupported = supportedModels.includes(model);
    console.log(
      `ğŸ”§ Tool support check - Provider: ${provider}, Model: ${model}, Supported: ${isSupported}`
    );
    return isSupported;
  }
}
